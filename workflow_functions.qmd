---
title: "workflow_functions"
format: html
editor: visual
---

# # run the analysis pipeline one step at a time

## load, filter and standardize data

```{r}
# load_and_prepare_data0 <- function(file_path, sample_size, replication) {
#   data <- fread(file_path)[N == sample_size & r == replication]
#   data <- as.data.frame(data) %>% select(-V1, -N, -r)  # Adjust 'V1' as needed based on the data loading method
#   data[ , -1] <- scale(data[ , -1])  # Standardize all columns except dependent variable 'y'
#   return(data)
# }
load_and_prepare_data <- function(sample_size, replication, corr, TE) {
  
  # Construct the file name based on the given conditions
  data_dir <- "simdata/"
  train_file <- paste0(data_dir, "train_n", sample_size, "_p75_corr", corr, "_TE", TE, ".RData")
  test_file  <- paste0(data_dir, "test_fortrain_n", sample_size, "_p75_corr", corr, "_TE", TE, ".RData")
  
  # Load the correct training dataset
  if (file.exists(train_file)) {
    load(train_file)  # This loads `df_train`
  } else {
    stop(paste("Training data file not found:", train_file))
  }
  
  # Load the correct test dataset
  if (file.exists(test_file)) {
    load(test_file)  # This loads `df_test`
  } else {
    stop(paste("Test data file not found:", test_file))
  }
  
  # Extract the correct replication
  if (replication > length(df_train) || replication > length(df_test)) {
    stop("Replication index exceeds available replications.")
  }
  
  # Select the appropriate replication
  train_data <- df_train[[replication]]
  test_data <- df_test[[replication]]
  
  return(list(train = train_data, test = test_data))
}
```


```{r}
data <- load_and_prepare_data(sample_size = 75, replication = 12, corr = 0.2, TE = "clustered")
train_data <- data$train
test_data <- data$test
train_means <- colMeans(train_data[, -1])
train_sds <- apply(train_data[, -1], 2, sd)
# Standardize training data
train_data[, -1] <- scale(train_data[, -1]) %>% as.data.frame()  # Standardize all columns except for 'y'
# Standardize test data using its own test statistics
test_data[, -1] <- scale(test_data[, -1]) %>% as.data.frame() # Standardize all columns except for 'y'
# Standardize test data using training statistics
test_data_scaledOnTrain <- data$test # Copy the original test data
test_data_scaledOnTrain[, -1] <- scale(test_data_scaledOnTrain[, -1], center = train_means, scale = train_sds) %>% as.data.frame() # Standardize all columns except for 'y'
```

```{r}
summary(test_data_scaledOnTrain)
```

## fit the reference model and save for projpred

```{r}
fit_reference_model <- function(data, seed = 123) {
  refm_fit <- brm(
    formula = y ~ .,
    data = data,
    family = gaussian(),
    prior = prior(horseshoe(), class = "b"),
    chains = 4,
    iter = 2000,
    backend = "cmdstanr",
    seed = seed
  )
  return(refm_fit)
}
```


```{r}
# Fit the reference model
refm_fit <- fit_reference_model(train_data)
  
# Save convergence info reference model and save the reference model summary
summ_ref <- summary(refm_fit)
summ_ref$fixed$div <- rstan::get_num_divergent(refm_fit$fit)
```

```{r}
summ_ref
```

## run cross-validation for variable selection

```{r}
run_projpred <- function(refm_fit, K, nterms_max = 16, seed = 123) {
  
  # get the reference model object
  refm_obj <- get_refmodel(refm_fit)
  
  cvvs <- cv_varsel(
    refm_obj,
    cv_method = "kfold",
    K = K,
    method = "forward",
    nclusters = 20,
    ndraws_pred = 400,
    nterms_max = nterms_max,
    verbose = FALSE,
    seed = seed
  )
  
  
  # Try to suggest size
  # Compute various suggest_size heuristics
  suggested_sizes <- list(
    default = tryCatch({
      suggest_size(cvvs, type = "upper")
    }, error = function(e) {
      cat("Error in suggest_size (default):", e$message, "\n")
      return(NA)
    }),
    lower = tryCatch({
      suggest_size(cvvs, type = "lower")
    }, error = function(e) {
      cat("Error in suggest_size (lower):", e$message, "\n")
      return(NA)
    }),
    thres_upper = tryCatch({
      suggest_size(cvvs, thres_elpd = -4, type = "upper")
    }, error = function(e) {
      cat("Error in suggest_size (thres_upper):", e$message, "\n")
      return(NA)
    }),
    thres_lower = tryCatch({
      suggest_size(cvvs, thres_elpd = -4, type = "lower")
    }, error = function(e) {
      cat("Error in suggest_size (thres_lower):", e$message, "\n")
      return(NA)
    })
  )
  
  
  ranking <- ranking(cvvs)
  summary_cvvs <- summary(cvvs)
  
  output_projpred <- list(
    suggested_sizes = suggested_sizes,
    ranking = ranking,
    summary_cvvs = summary_cvvs,
    cvvs = cvvs
  )
  
  return(output_projpred)
}

```


```{r}
ppvs_out <- run_projpred(refm_fit, K = 5, nterms_max = 16)
```

```{r}
ppvs_out$cvvs <- NULL
  
# Add selection based on projpred to reference model summary
df_out <- summ_ref$fixed
for (heuristic in names(ppvs_out$suggested_sizes)) {
  suggested_size <- ppvs_out$suggested_sizes[[heuristic]]
  if (is.na(suggested_size)) {
    df_out[[paste0("selected_pred_ppvs_", heuristic)]] <- NA
  } else {
    selected_pred_ppvs <- head(ppvs_out$ranking[["fulldata"]], suggested_size)
    df_out[[paste0("selected_pred_ppvs_", heuristic)]] <- 0
    df_out[[paste0("selected_pred_ppvs_", heuristic)]][rownames(df_out) %in% selected_pred_ppvs] <- 1
    df_out[[paste0("selected_pred_ppvs_", heuristic)]][rownames(df_out) == "Intercept"] <- 1
  }
}
```

```{r}
head(df_out)
```


## compute predictions and pmse
```{r}
# Compute predictions for each heuristic
prj_linpred_results <- data.frame(y = test_data$y)  # Start with the true y values
refm_obj <- get_refmodel(refm_fit)

for (heuristic in names(ppvs_out$suggested_sizes)) {
  suggested_size <- ppvs_out$suggested_sizes[[heuristic]]
  if (is.na(suggested_size)) {
  # Add an NA column for the heuristic
  prj_linpred_results[[paste0("prediction_", heuristic)]] <- NA
  next
  }
  
  # Extract selected predictors
  predictors_final <- head(ppvs_out$ranking[["fulldata"]], suggested_size)
  
  # Generate projections
  prj <- project(
  refm_obj,
  predictor_terms = predictors_final,
  verbose = FALSE
  )
  
  # Predict on test data
  prj_linpred <- proj_linpred(
  prj,
  newdata = test_data[, -1], # drop the dependent 'y' column
  integrated = TRUE
  )
  prj_linpred_testScaledOnTrain <- proj_linpred(
  prj,
  newdata = test_data_scaledOnTrain[, -1], # drop the dependent 'y' column
  integrated = TRUE
  )
  
  # Add predictions as a new column for the heuristic
  prj_linpred_results[[paste0("pred_", heuristic)]] <- as.vector(prj_linpred$pred)
  prj_linpred_results[[paste0("pred_", heuristic, "_testScaledOnTrain")]] <- as.vector(prj_linpred_testScaledOnTrain$pred)
}
```

```{r}
head(prj_linpred_results)
```

```{r}
mean((prj_linpred_results$y - prj_linpred_results$pred_default)^2)
mean((prj_linpred_results$y - prj_linpred_results$pred_default_testScaledOnTrain)^2)
```

```{r}
mean((prj_linpred_results$y - prj_linpred_results$pred_thres_upper)^2)
mean((prj_linpred_results$y - prj_linpred_results$pred_thres_upper_testScaledOnTrain)^2)
```

```{r}
mean((prj_linpred_results$y - prj_linpred_results$pred_thres_lower)^2)
mean((prj_linpred_results$y - prj_linpred_results$pred_thres_lower_testScaledOnTrain)^2)
```
### alternatively do point prediction by hand with mean intercepts and betas
```{r}
# prj_mat <- as.matrix(prj)
# library(posterior)
# prj_drws <- as_draws_matrix(prj_mat)
# prj_smmry <- summarize_draws(prj_drws, "mean")
# # Coerce to a `data.frame` because pkgdown versions > 1.6.1 don't print the
# # tibble correctly:
# prj_smmry <- as.data.frame(prj_smmry)
# print(prj_smmry, digits = 1)
```

```{r}
# Compute predictions for each heuristic
prj_linpred_results2 <- data.frame(y = test_data$y)  # Start with the true y values
refm_obj <- get_refmodel(refm_fit)
for (heuristic in names(ppvs_out$suggested_sizes)) {
  suggested_size <- ppvs_out$suggested_sizes[[heuristic]]
  if (is.na(suggested_size)) {
  # Add an NA column for the heuristic
  prj_linpred_results2[[paste0("prediction_", heuristic)]] <- NA
  next
  }
  
  # Extract selected predictors
  predictors_final <- head(ppvs_out$ranking[["fulldata"]], suggested_size)
  
  # Generate projections
  prj <- project(
  refm_obj,
  predictor_terms = predictors_final,
  verbose = FALSE
  )
  
  prj_mat <- as.matrix(prj)
  prj_drws <- as_draws_matrix(prj_mat)
  prj_smmry <- summarize_draws(prj_drws, "mean")

  # Predict on test data
  prj_linpred2 <- prj_smmry$mean[1] + as.matrix(test_data[, predictors_final]) %*% prj_smmry$mean[-c(1, nrow(prj_smmry))]
  prj_linpred2_testScaledOnTrain <- prj_smmry$mean[1] + as.matrix(test_data_scaledOnTrain[, predictors_final]) %*% prj_smmry$mean[-c(1, nrow(prj_smmry))]
  
  # Add predictions as a new column for the heuristic
  prj_linpred_results2[[paste0("pred_", heuristic)]] <- prj_linpred2
  prj_linpred_results2[[paste0("pred_", heuristic, "_testScaledOnTrain")]] <- prj_linpred2_testScaledOnTrain
}
```


```{r}
head(prj_linpred_results2)
```

```{r}
mean((prj_linpred_results2$y - prj_linpred_results2$pred_default)^2)
mean((prj_linpred_results2$y - prj_linpred_results2$pred_default_testScaledOnTrain)^2)
```

```{r}
mean((prj_linpred_results2$y - prj_linpred_results2$pred_thres_upper)^2)
mean((prj_linpred_results2$y - prj_linpred_results2$pred_thres_upper_testScaledOnTrain)^2)
```

```{r}
mean((prj_linpred_results2$y - prj_linpred_results2$pred_thres_lower)^2)
mean((prj_linpred_results2$y - prj_linpred_results2$pred_thres_lower_testScaledOnTrain)^2)
```

```{r}
sum(round(prj_linpred_results2$pred_default, 10) == round(prj_linpred_results$pred_default, 10))
sum(round(prj_linpred_results2$pred_default_testScaledOnTrain, 10) == round(prj_linpred_results$pred_default_testScaledOnTrain, 10))
```
```{r}
sum(prj_linpred_results2$pred_default == prj_linpred_results$pred_default)
sum(prj_linpred_results2$pred_default_testScaledOnTrain == prj_linpred_results$pred_default_testScaledOnTrain)
```


## alternatively do linear predictor with uncertainty by hand (could be how prj_linpred works)
```{r}
prj_drws[, -nrow(prj_smmry)] %*% t(cbind(1, test_data[, predictors_final]))
```


```{r}
sum(round(colMeans(prj_drws[, -nrow(prj_smmry)] %*% t(cbind(1, test_data[, predictors_final]))), 10) == round(prj_linpred_results$pred_thres_lower, 10))
```
```{r}
sum(colMeans(prj_drws[, -nrow(prj_smmry)] %*% t(cbind(1, test_data[, predictors_final]))) == prj_linpred_results$pred_thres_lower)
```

```{r}
sum(colMeans(prj_drws[, -nrow(prj_smmry)] %*% t(cbind(1, test_data[, predictors_final]))) == prj_linpred_results2$pred_thres_lower)
sum(round(colMeans(prj_drws[, -nrow(prj_smmry)] %*% t(cbind(1, test_data[, predictors_final]))), 10) == round(prj_linpred_results2$pred_thres_lower, 10))
```
```{r}
summary(prj_drws)
```

```{r}
sum(colMeans(prj_drws[, -nrow(prj_smmry)] %*% t(cbind(1, test_data[, predictors_final]))) == colMeans(proj_linpred(prj, newdata = test_data[, -1], integrated = FALSE)$pred))
```



```{r}
sum(colMeans(proj_linpred(prj, newdata = test_data[, -1], integrated = FALSE)$pred) == proj_linpred(prj, newdata = test_data[, -1], integrated = TRUE)$pred)
```
```{r}
sum(colMeans(proj_linpred(prj, newdata = test_data[, -1], integrated = FALSE)$pred) == prj_linpred$pred)
```


```{r}
# # Generate projections
# prj_allrefdraws <- project(
# refm_obj,
# predictor_terms = predictors_final,
# ndraws = NULL,
# verbose = FALSE
# )
# takes longer, not worth it
# # Predict on test data
# prj_linpred_allrefdraws <- proj_linpred(
# prj_allrefdraws,
# newdata = test_data[, -1], # drop the dependent 'y' column
# integrated = TRUE
# )

# prj_mat1 <- as.matrix(project(
# refm_obj,
# predictor_terms = predictors_final,
# verbose = FALSE
# ))
# 
# prj_mat2 <- as.matrix(project(
# refm_obj,
# predictor_terms = predictors_final,
# verbose = FALSE
# ))
# 
# all.equal(prj_mat1, prj_mat2)
# only 400 draws from the reference model are drawn, but somehow the resulting matrices are the same without setting seed
```


```{r}
  # Compute predictions for each heuristic
  
  # Create an empty list to store predictions from all heuristics
  prj_pred_results_list <- list()
  
  # Store true test y-values (for easy PMSE calculation later)
  prj_pred_results_list[["y"]] <- test_data$y 
  
  for (heuristic in names(ppvs_out$suggested_sizes)) {
    suggested_size <- ppvs_out$suggested_sizes[[heuristic]]
    
    if (is.na(suggested_size)) {
      prj_pred_results_list[[heuristic]] <- NULL
      next
    }
    
    # Extract selected predictors
    predictors_final <- head(ppvs_out$ranking[["fulldata"]], suggested_size)
    
    # Get the reference model object
    refm_obj <- get_refmodel(refm_fit)
    
    # Generate projections
    prj <- project(
      refm_obj,
      predictor_terms = predictors_final,
      verbose = FALSE
    )
    prj_mat <- as.matrix(prj)
    prj_drws <- as_draws_matrix(prj_mat)
    prj_smmry <- summarize_draws(prj_drws, "mean")
    
    # Predict on test data
    
    ## Point prediction
    prj_point_pred <- prj_smmry$mean[1] + as.matrix(test_data[, predictors_final]) %*% prj_smmry$mean[-c(1, nrow(prj_smmry))]
    prj_point_pred_testScaledOnTrain <- prj_smmry$mean[1] + as.matrix(test_data_scaledOnTrain[, predictors_final]) %*% prj_smmry$mean[-c(1, nrow(prj_smmry))]
    
    ## Linear predictor with uncertainty
    prj_linpred <- proj_linpred(
      prj,
      newdata = test_data[, -1], # drop the dependent 'y' column
      integrated = TRUE
    )
    prj_linpred_testScaledOnTrain <- proj_linpred(
      prj,
      newdata = test_data_scaledOnTrain[, -1], # drop the dependent 'y' column
      integrated = TRUE
    )
    
    ## Predictive distribution for a new observation
    prj_pred <- proj_predict(prj, newdata = test_data[, -1], .seed = 123)
    prj_pred_testScaledOnTrain <- proj_predict(prj, newdata = test_data_scaledOnTrain[, -1], .seed = 123)
    
    # Store results for this heuristic inside prj_pred_results_list
    prj_pred_results_list[[heuristic]] <- list(
      point_pred = as.vector(prj_point_pred),
      point_pred_testScaledOnTrain = as.vector(prj_point_pred_testScaledOnTrain),
      linpred = as.vector(prj_linpred$pred),
      linpred_testScaledOnTrain = as.vector(prj_linpred_testScaledOnTrain$pred),
      prj_pred = prj_pred,  # 400 × 1000 posterior predictive samples
      prj_pred_testScaledOnTrain = prj_pred_testScaledOnTrain  # 400 × 1000 predictions scaled on training stats
    )
  }
```
```{r}
# Create an empty list to store predictions from all heuristics
prj_pred_results_list <- list()

# Store true test y-values (for easy PMSE calculation later)
prj_pred_results_list[["y"]] <- test_data$y 

# Step 1: Group heuristics by suggested size
# Convert suggested sizes to a named vector
suggested_sizes_vec <- unlist(ppvs_out$suggested_sizes)

# Remove NA values to avoid issues
suggested_sizes_vec <- suggested_sizes_vec[!is.na(suggested_sizes_vec)]

# Create a proper grouping: heuristics mapped to suggested sizes
heuristic_groups <- split(names(suggested_sizes_vec), suggested_sizes_vec)

# Step 2: Loop over unique suggested sizes
prj_cache <- list()  # Cache projections to avoid recomputation

for (suggested_size in unique(ppvs_out$suggested_sizes)) {
    
    if (is.na(suggested_size)) {
        # If a heuristic fails, store NULL for all heuristics in that group
        for (heuristic in heuristic_groups[[as.character(suggested_size)]]) {
            prj_pred_results_list[[heuristic]] <- NULL
        }
        next
    }
    
    # Extract selected predictors (same for all heuristics in this group)
    predictors_final <- head(ppvs_out$ranking[["fulldata"]], suggested_size)

    # Check if projection has already been computed
    predictors_key <- paste(sort(predictors_final), collapse = "_")

    if (!predictors_key %in% names(prj_cache)) {
        # Compute projection only once per unique predictor set
        prj_cache[[predictors_key]] <- project(
            refm_obj,
            predictor_terms = predictors_final,
            verbose = FALSE
        )
    }

    # Reuse the stored projection
    prj <- prj_cache[[predictors_key]]

    # Convert projection to draws
    prj_mat <- as.matrix(prj)
    prj_drws <- as_draws_matrix(prj_mat)
    prj_smmry <- summarize_draws(prj_drws, "mean")

    # Predictions
    prj_point_pred <- prj_smmry$mean[1] + as.matrix(test_data[, predictors_final]) %*% prj_smmry$mean[-c(1, nrow(prj_smmry))]
    prj_point_pred_testScaledOnTrain <- prj_smmry$mean[1] + as.matrix(test_data_scaledOnTrain[, predictors_final]) %*% prj_smmry$mean[-c(1, nrow(prj_smmry))]

    prj_linpred <- proj_linpred(prj, newdata = test_data[, -1], integrated = TRUE)
    prj_linpred_testScaledOnTrain <- proj_linpred(prj, newdata = test_data_scaledOnTrain[, -1], integrated = TRUE)

    prj_pred <- proj_predict(prj, newdata = test_data[, -1], .seed = 123)
    prj_pred_testScaledOnTrain <- proj_predict(prj, newdata = test_data_scaledOnTrain[, -1], .seed = 123)

    # Step 3: Assign results to all heuristics in this group
    for (heuristic in heuristic_groups[[as.character(suggested_size)]]) {
        prj_pred_results_list[[heuristic]] <- list(
            point_pred = as.vector(prj_point_pred),
            point_pred_testScaledOnTrain = as.vector(prj_point_pred_testScaledOnTrain),
            linpred = as.vector(prj_linpred$pred),
            linpred_testScaledOnTrain = as.vector(prj_linpred_testScaledOnTrain$pred),
            prj_pred = prj_pred,  # 400 × 1000 posterior predictive samples
            prj_pred_testScaledOnTrain = prj_pred_testScaledOnTrain
        )
    }
}
```






