---
title: "study2Analysis"
format: html
---
```{r}
library(stringr)
library(dplyr)
library(purrr)
library(ggplot2)
library(tidyr)
```

# ppvs all_df_out
```{r}
# setwd("D:/projpred_vs_spikeslab")
# List all df_out files
df_out_files_study2 <- list.files("output/output_study2/ppvs/df_out/", pattern = "^df_out_", full.names = TRUE)

# Function to read and process a single df_out file
read_df_out <- function(file_path) {
  load(file_path)  # Loads df_out
  
  # Extract metadata from filename
  metadata <- tibble(
    N = as.numeric(str_extract(file_path, "(?<=df_out_N)\\d+")),
    corr = as.numeric(str_extract(file_path, "(?<=_corr)0\\.\\d+")),
    TE = str_extract(file_path, "(?<=_TE)[a-z]+"),
    r = as.numeric(str_extract(file_path, "(?<=_r)\\d+"))
  )
  
  # Convert df_out to tibble, keeping rownames as a column
  df_out <- as_tibble(df_out, rownames = "Variable")
  
  # Remove the intercept row
  df_out <- df_out %>% filter(Variable != "Intercept")
  
  # Attach metadata columns to df_out
  df_out <- df_out %>% mutate(N = metadata$N, corr = metadata$corr, TE = metadata$TE, r = metadata$r)
  
  return(df_out)
}

# Apply function to all files and combine into one big dataframe
all_df_out_study2 <- bind_rows(lapply(df_out_files_study2, read_df_out))

# View structure of the final dataframe
glimpse(all_df_out_study2)
head(all_df_out_study2)
```


# ppvs convergence
```{r}
# number of nonconver reps
all_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  summarise(n_div = unique(div), .groups = "drop") %>%
  group_by(N, corr, TE) %>%
  summarise(
    mean_div = mean(n_div),
    median_div = median(n_div),
    sd_div = sd(n_div),
    min_div = min(n_div),
    max_div = max(n_div),
    .groups = "drop"
  )

# perc of nonconver reps
all_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  summarise(perc_div = unique(div) * 100 / 4000, .groups = "drop") %>%
  group_by(N, corr, TE) %>%
  summarise(
    mean_perc = mean(perc_div),
    median_perc = median(perc_div),
    sd_perc = sd(perc_div),
    min_perc = min(perc_div),
    max_perc = max(perc_div),
    .groups = "drop"
  )

# looking at the conditions which seem to be the most problematic
all_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  summarise(perc_div = unique(div) * 100 / 4000, .groups = "drop") %>%
  filter(N == 50, corr == 0.2, TE == "mixed") %>%
  arrange(desc(perc_div))

all_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  summarise(perc_div = unique(div) * 100 / 4000, .groups = "drop") %>%
  filter(N == 50, corr == 0.2, TE == "clustered") %>%
  arrange(desc(perc_div))

# problematic reps
all_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  summarise(
    n_div = unique(div),
    perc_div = unique(div) * 100 / 4000, 
    n_Rhat = sum(Rhat >= 1.01),
    n_ESS = sum(Bulk_ESS <= 400),
    .groups = "drop"
  ) %>%
  filter(n_Rhat != 0 | n_ESS != 0 | perc_div > 10) %>%
  group_by(N, corr, TE) %>%
  summarise(n_reps = n(), .groups = "drop")

# reps that didn't converge
all_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  summarise(n_div = unique(div),
            perc_div = unique(div) * 100 / 4000,
            n_Rhat = sum(Rhat >= 1.1),
            .groups = "drop") %>%
  filter(n_Rhat != 0 | perc_div > 10) %>%
  select(N, corr, TE, r)

# save problematic reps
problematic_reps_study2 <- all_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  summarise(
    n_div = unique(div),
    perc_div = unique(div) * 100 / 4000, 
    n_Rhat = sum(Rhat >= 1.01),
    n_ESS = sum(Bulk_ESS <= 400),
    .groups = "drop"
  ) %>%
  filter(n_Rhat != 0 | n_ESS != 0 | perc_div > 10) %>%
  select(N, corr, TE, r)

# save reps that didn't converge
nonconver_reps_study2 <- all_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  summarise(n_div = unique(div),
            perc_div = unique(div) * 100 / 4000,
            n_Rhat = sum(Rhat >= 1.1),
            .groups = "drop") %>%
  filter(n_Rhat != 0 | perc_div > 10) %>%
  select(N, corr, TE, r)

```


# ppvs heuristics checking
```{r}
# thres_lower vs thres_upper heuristics

# for how many replications did thres_upper and thres_lower just gave the same selection of predictors (where they aren't the same, most of the time they are just NAs where at least one of them failed)
all_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  summarise(
    agreement = all(selected_pred_ppvs_thres_upper == selected_pred_ppvs_thres_lower),
    .groups = "drop"
  ) %>%
  group_by(N, corr, TE) %>%
  summarise(
    n_agree = sum(agreement, na.rm = TRUE),               # how many replications had full agreement
    n_NA = sum(is.na(agreement)),
    total_replications = n(),
    .groups = "drop"
  )
```

# ppvs rates
```{r}
# Function to determine true predictors based on TE
get_true_predictors_study2 <- function(TE) {
  if (TE == "clustered") {
    return(paste0("X", 1:15))
  } else if (TE == "mixed") {
    return(paste0("X", seq(1, 75, 5)))
  } else {
    stop("Unexpected TE value!")
  }
}

compute_rates_from_df_study2 <- function(df, heuristics) {

  true_predictors <- get_true_predictors_study2(df$TE[1])

  results <- list()

  for (heuristic in heuristics) {
    # Check if the heuristic failed (all values are NA)
    if (all(is.na(df[[heuristic]]))) {
      # Return NA-filled results for this heuristic
      results[[heuristic]] <- tibble(
        N = df$N[1],
        corr = df$corr[1],
        TE = df$TE[1],
        r = df$r[1],
        heuristic = heuristic,
        true_inclusion_rate = NA,
        false_inclusion_rate = NA,
        false_exclusion_rate = NA,
        true_exclusion_rate = NA
      )
      next  # Skip to the next heuristic
    }

    # Identify selected predictors for this heuristic
    selected_predictors <- df$Variable[df[[heuristic]] == 1]

    # Compute inclusion/exclusion counts
    true_inclusion <- sum(selected_predictors %in% true_predictors)
    false_inclusion <- sum(!(selected_predictors %in% true_predictors))
    false_exclusion <- sum(!(true_predictors %in% selected_predictors))
    true_exclusion <- sum(!(df$Variable %in% selected_predictors) & !(df$Variable %in% true_predictors))

    # Compute rates
    true_inclusion_rate <- true_inclusion / length(true_predictors)
    false_inclusion_rate <- false_inclusion / (nrow(df) - length(true_predictors))
    false_exclusion_rate <- false_exclusion / length(true_predictors)
    true_exclusion_rate <- true_exclusion / (nrow(df) - length(true_predictors))

    # Store results in a long format
    results[[heuristic]] <- tibble(
      N = df$N[1],
      corr = df$corr[1],
      TE = df$TE[1],
      r = df$r[1],
      heuristic = heuristic,
      true_inclusion_rate = true_inclusion_rate,
      false_inclusion_rate = false_inclusion_rate,
      false_exclusion_rate = false_exclusion_rate,
      true_exclusion_rate = true_exclusion_rate
    )
  }

  return(bind_rows(results))
}

```

```{r cache=TRUE}
heuristics <- c("selected_pred_ppvs_default", 
                "selected_pred_ppvs_lower", 
                "selected_pred_ppvs_thres_upper", 
                "selected_pred_ppvs_thres_lower")  # Add more heuristics if needed

all_rates_long_study2 <- all_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  group_split() %>%
  map_dfr(~ compute_rates_from_df_study2(.x, heuristics))

all_rates_long_study2 <- all_rates_long_study2 %>%
  mutate(heuristic = str_remove(heuristic, "^selected_pred_ppvs_"))

glimpse(all_rates_long_study2)
head(all_rates_long_study2)
```

# ppvs NA visualization
```{r}
# NA is where the suggest_size function fails, so this holds also for other rates and later the pmse and coverage percentage
summary_na_study2 <- all_rates_long_study2 %>%
  anti_join(nonconver_reps_study2, by  = c("N", "corr", "TE", "r")) %>%
  group_by(N, corr, TE, heuristic) %>%
  summarise(
    na_count = sum(is.na(true_inclusion_rate)),  # Count how many replications failed
    .groups = "drop"
  )
summary_na_study2
```

```{r}
ggplot(summary_na_study2, aes(x = heuristic, y = na_count, fill = factor(corr))) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_grid(N ~ TE) +
  labs(
    title = "Number of NAs per Heuristic and Condition",
    x = "Heuristic",
    y = "NA Count",
    fill = "corr"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
# ssvs rates
```{r}
# setwd("D:/projpred_vs_spikeslab")
# List all ssvs_df_out files
ssvs_files_study2 <- list.files("output/output_study2/ssvs/ssvs_df_out/", pattern = "^ssvs_N", full.names = TRUE)

# Extract condition details from filenames
ssvs_conditions <- tibble(
  file = ssvs_files_study2,
  N = as.numeric(str_extract(ssvs_files_study2, "(?<=ssvs_N)\\d+")),
  corr = as.numeric(str_extract(ssvs_files_study2, "(?<=_corr)0\\.\\d+")),
  TE = str_extract(ssvs_files_study2, "(?<=_TE)[a-z]+"),
  r = as.numeric(str_extract(ssvs_files_study2, "(?<=_r)\\d+"))
)

# Function to load and append metadata
load_ssvs_df_out <- function(file_path) {
  load(file_path)  # This loads `ssvs_df_out`
  
  # Extract condition metadata
  condition <- ssvs_conditions %>% filter(file == file_path)
  
  # Remove the intercept row
  ssvs_df_out <- ssvs_df_out %>% filter(Variable != "Intercept")
  
  # Add metadata columns (N, corr, TE, r)
  ssvs_df_out <- ssvs_df_out %>%
    mutate(N = condition$N,
           corr = condition$corr,
           TE = condition$TE,
           r = condition$r)
  
  return(ssvs_df_out)
}

# Apply function to all files and combine them into one dataframe
all_ssvs_df_out_study2 <- bind_rows(lapply(ssvs_files_study2, load_ssvs_df_out))
head(all_ssvs_df_out_study2)
```


```{r cache=TRUE}
compute_ssvs_rates_study2 <- function(df) {

  true_predictors <- get_true_predictors_study2(df$TE[1])

  # Identify selected predictors
  selected_predictors <- df$Variable[df$selected_pred_ssvs == 1]

  # Compute counts
  true_inclusion <- sum(selected_predictors %in% true_predictors)
  false_inclusion <- sum(!(selected_predictors %in% true_predictors))
  false_exclusion <- sum(!(true_predictors %in% selected_predictors))
  true_exclusion <- sum(!(df$Variable %in% selected_predictors) & !(df$Variable %in% true_predictors))

  # Compute rates
  true_inclusion_rate <- true_inclusion / length(true_predictors)
  false_inclusion_rate <- false_inclusion / (nrow(df) - length(true_predictors))
  false_exclusion_rate <- false_exclusion / length(true_predictors)
  true_exclusion_rate <- true_exclusion / (nrow(df) - length(true_predictors))

  return(tibble(
    N = df$N[1],
    corr = df$corr[1],
    TE = df$TE[1],
    r = df$r[1],
    true_inclusion_rate = true_inclusion_rate,
    false_inclusion_rate = false_inclusion_rate,
    false_exclusion_rate = false_exclusion_rate,
    true_exclusion_rate = true_exclusion_rate
  ))
}


all_ssvs_rates_study2 <- all_ssvs_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  group_split() %>%
  map_dfr(compute_ssvs_rates_study2)

print(all_ssvs_rates_study2)

```

# combine rates
```{r}
# Add method column to both datasets
all_ssvs_rates_study2 <- all_ssvs_rates_study2 %>%
  mutate(heuristic = "SSVS")  # Ensure heuristic column exists

# exclude non-converged replications in ppvs dataset
all_rates_long_study2 <- all_rates_long_study2 %>%
  anti_join(nonconver_reps_study2, by = c("N", "corr", "TE", "r"))

# Combine both datasets
combined_rates_study2 <- bind_rows(all_rates_long_study2, all_ssvs_rates_study2)

combined_rates_study2 <- combined_rates_study2 %>%
  mutate(heuristic = factor(heuristic))

# View structure to check
glimpse(combined_rates_study2)
head(combined_rates_study2)
```

# counts
```{r cache=TRUE}
# counts, number of selected variables by heuristic
ppvs_counts_study2 <- all_df_out_study2 %>%
  anti_join(nonconver_reps_study2, by = c("N", "corr", "TE", "r")) %>%
  group_by(N, corr, TE, r) %>%
  summarise(default = sum(selected_pred_ppvs_default == 1),
            lower = sum(selected_pred_ppvs_lower == 1),
            thres_upper = sum(selected_pred_ppvs_thres_upper == 1), 
            thres_lower = sum(selected_pred_ppvs_thres_lower == 1),
            .groups = "drop") %>%
  pivot_longer(cols = c(default, lower, thres_upper, thres_lower), 
               names_to = "heuristic", 
               values_to = "n_selected")

ssvs_counts_study2 <- all_ssvs_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  summarise(n_selected = sum(selected_pred_ssvs == 1),
            .groups = "drop") %>%
  mutate(heuristic = "SSVS")

combined_counts_study2 <- bind_rows(ppvs_counts_study2, ssvs_counts_study2)
combined_counts_study2
```


# true and false inclusion rates visualization
```{r}
combined_rates_study2 %>%
  ggplot(aes(x = factor(N), y = true_inclusion_rate, fill = heuristic)) +
  geom_boxplot(alpha = 0.7) +
  facet_grid(TE ~ corr) +
  theme_minimal() +
  labs(title = "True Inclusion Rate cross conditions",
       x = "N",
       y = "True Inclusion Rate") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

combined_rates_study2 %>%
  ggplot(aes(x = factor(N), y = false_inclusion_rate, fill = heuristic)) +
  geom_boxplot(alpha = 0.7) +
  facet_grid(TE ~ corr) +
  theme_minimal() +
  labs(title = "False Inclusion Rate cross conditions",
       x = "N",
       y = "False Inclusion Rate") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# counts visualization
```{r}
combined_counts_study2 %>%
  filter(heuristic != "thres_lower") %>%
  ggplot(aes(x = factor(corr), y = n_selected, fill = heuristic)) +
  geom_boxplot(alpha = 0.7) +  
  facet_grid(~ TE) +  
  labs(title = "Counts of selected predictors Across Conditions", 
       x = "corr", 
       y = "Count") +
  theme_minimal()
```

```{r}
# filtering the replications where the lower threshold worked
combined_counts_study2 %>%
  # Then filter for reps where 'lower' heuristic worked
  semi_join(
    combined_counts_study2 %>%
      filter(heuristic == "lower", !is.na(n_selected)) %>%
      select(N, corr, TE, r),
    by = c("N", "corr", "TE", "r")
  ) %>%

  filter(heuristic != "thres_lower") %>%
  ggplot(aes(x = factor(corr), y = n_selected, fill = heuristic)) +
  geom_boxplot(alpha = 0.7) +  
  facet_grid(~ TE) +  
  labs(title = "Counts of selected predictors Across Conditions", 
       x = "corr", 
       y = "Count") +
  theme_minimal()
```


```{r}
# numbers of interncept intercept models for PPVS-default and SSVS across conditions
combined_counts_study2 %>%
  filter(heuristic %in% c("default", "SSVS")) %>%
  group_by(N, corr, TE, heuristic) %>%
  summarise(num_0_selected = sum(n_selected == 0, na.rm = TRUE), .groups = "drop")
```


# ppvs rates by effect size
```{r cache=TRUE}
assign_effect_sizes_study2 <- function(TE) {
  if (TE == "clustered") {
    c(rep("large", 5), rep("medium", 5), rep("small", 5), rep("null", 60))
  } else if (TE == "mixed") {
    true_predictors <- seq(1, 75, 5)  # X1, X6, X11, ..., X71
    effect_sizes <- rep("null", 75)
    effect_sizes[true_predictors[1:5]] <- "large"
    effect_sizes[true_predictors[6:10]] <- "medium"
    effect_sizes[true_predictors[11:15]] <- "small"
    return(effect_sizes)
  } else {
    stop("Unknown TE value")
  }
}

compute_rates_by_effect_size_study2 <- function(df, heuristics) {
  results <- list()

  for (heuristic in heuristics) {
    if (all(is.na(df[[heuristic]]))) {
      # Return NA rows for each effect size category
      effect_levels <- c("large", "medium", "small", "null")
      na_results <- tibble(
        N = df$N[1],
        corr = df$corr[1],
        TE = df$TE[1],
        r = df$r[1],
        heuristic = heuristic,
        effect_size = effect_levels,
        inclusion_rate = NA_real_
      )
      results[[heuristic]] <- na_results
      next
    }

    # Assign effect sizes to predictors
    effect_sizes <- assign_effect_sizes_study2(df$TE[1])
    df$effect_size <- effect_sizes

    # Compute inclusion rate for each effect size group
    inclusion_by_group <- df %>%
      mutate(selected = df[[heuristic]]) %>%
      group_by(effect_size) %>%
      summarise(
        inclusion_rate = mean(selected, na.rm = TRUE),
        .groups = "drop"
      ) %>%
      mutate(
        N = df$N[1],
        corr = df$corr[1],
        TE = df$TE[1],
        r = df$r[1],
        heuristic = heuristic
      ) %>%
      select(N, corr, TE, r, heuristic, effect_size, inclusion_rate)

    results[[heuristic]] <- inclusion_by_group
  }

  return(bind_rows(results))
}

ppvs_rates_by_effect_size_study2 <- all_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  group_split() %>%
  map_dfr(~ compute_rates_by_effect_size_study2(.x, heuristics))

ppvs_rates_by_effect_size_study2 <- ppvs_rates_by_effect_size_study2 %>%
  anti_join(nonconver_reps_study2, by = c("N", "corr", "TE", "r"))

ppvs_rates_by_effect_size_study2 <- ppvs_rates_by_effect_size_study2 %>%
  mutate(heuristic = str_remove(heuristic, "^selected_pred_ppvs_"))
ppvs_rates_by_effect_size_study2
```


# ssvs rates by effect size
```{r cache=TRUE}
compute_ssvs_rates_by_effect_size_study2 <- function(df) {
  # Determine true predictors
  true_predictors <- get_true_predictors_study2(df$TE[1])
  
  # Assign effect size labels (large, medium, small, null)
  df <- df %>%
    mutate(
      effect_size = case_when(
        Variable %in% true_predictors ~ rep(c("large", "medium", "small"), each = 5)[match(Variable, true_predictors)],
        TRUE ~ "null"
      )
    )

  # Count total predictors per group (denominator for inclusion rate)
  effect_size_counts <- df %>%
    group_by(effect_size) %>%
    summarise(total = n(), .groups = "drop")
  
  # Count how many predictors were selected in each group
  selected_counts <- df %>%
    filter(selected_pred_ssvs == 1) %>%
    group_by(effect_size) %>%
    summarise(selected = n(), .groups = "drop")
  
  # Join and calculate inclusion rates per effect size group
  rates_by_effect_size <- left_join(effect_size_counts, selected_counts, by = "effect_size") %>%
    mutate(
      selected = replace_na(selected, 0),
      inclusion_rate = selected / total,
      N = df$N[1],
      corr = df$corr[1],
      TE = df$TE[1],
      r = df$r[1],
      heuristic = "SSVS"
    ) %>%
    select(N, corr, TE, r, effect_size, inclusion_rate, heuristic)

  return(rates_by_effect_size)
}

all_ssvs_rates_by_effect_size_study2 <- all_ssvs_df_out_study2 %>%
  group_by(N, corr, TE, r) %>%
  group_split() %>%
  map_dfr(compute_ssvs_rates_by_effect_size_study2)
all_ssvs_rates_by_effect_size_study2
```

# combine rates by effect size
```{r}
# Combine both datasets
combined_rates_by_effect_size_study2 <- bind_rows(ppvs_rates_by_effect_size_study2, all_ssvs_rates_by_effect_size_study2)

# View structure to check
glimpse(combined_rates_by_effect_size_study2)
head(combined_rates_by_effect_size_study2)
```
# combined rates by effect size visualization

```{r}
combined_rates_by_effect_size_study2 %>%
  filter(!(heuristic %in% c("lower", "thres_lower", "thres_upper"))) %>%
  mutate(
    effect_size = factor(effect_size, levels = c("large", "medium", "small", "null")),
    method = dplyr::recode(heuristic,
                    "default" = "PPVS - SE heuristic upper bound",
                    "SSVS" = "SSVS"),
    corr = as.factor(corr)  # convert to factor to use with labeller
  ) %>%
  #filter(effect_size == "small") %>%
  ggplot(aes(x = factor(TE), y = inclusion_rate, fill = method, col = method)) + 
  geom_boxplot(alpha = 0.7) +
  stat_summary(fun = mean, geom = "crossbar", width = 0.8, color = "black", fatten = 0.9, position = position_dodge(width = 0.75)) +
  facet_grid(corr + N ~ effect_size) +
  theme_minimal() +
  labs(
    title = "Inclusion Rate for effect size small across conditions",
    x = "Pattern of true effects",
    y = "Inclusion Rate",
    fill = "Method",
    color = "Method"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    strip.background = element_rect(colour = "black", fill = "white")
  )

combined_rates_by_effect_size_study2 %>%
  filter(!(heuristic %in% c("lower", "thres_lower", "thres_upper"))) %>%
  mutate(
    effect_size = factor(effect_size, levels = c("large", "medium", "small", "null")),
    method = dplyr::recode(heuristic,
                    "default" = "PPVS - SE heuristic upper bound",
                    "SSVS" = "SSVS"),
    corr = as.factor(corr)  # convert to factor to use with labeller
  ) %>%
  filter(!(effect_size %in% c("small", "null")), corr != 0.8) %>%
  ggplot(aes(x = factor(TE), y = inclusion_rate, fill = method, col = method)) + 
  geom_boxplot(alpha = 0.7) +
  stat_summary(fun = mean, geom = "crossbar", width = 0.8, color = "black", fatten = 0.9, position = position_dodge(width = 0.75)) +
  facet_grid(corr + N ~ effect_size) +
  theme_minimal() +
  labs(
    title = "Inclusion Rate for effect size small across conditions",
    x = "Pattern of true effects",
    y = "Inclusion Rate",
    fill = "Method",
    color = "Method"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    strip.background = element_rect(colour = "black", fill = "white")
  )

combined_rates_by_effect_size_study2 %>%
  filter(!(heuristic %in% c("lower", "thres_lower", "thres_upper"))) %>%
  mutate(
    effect_size = factor(effect_size, levels = c("large", "medium", "small", "null")),
    method = dplyr::recode(heuristic,
                    "default" = "PPVS - SE heuristic upper bound",
                    "SSVS" = "SSVS"),
    corr = as.factor(corr)  # convert to factor to use with labeller
  ) %>%
  filter(!(effect_size %in% c("small", "null")), corr != 0.2) %>%
  ggplot(aes(x = factor(TE), y = inclusion_rate, fill = method, col = method)) + 
  geom_boxplot(alpha = 0.7) +
  stat_summary(fun = mean, geom = "crossbar", width = 0.8, color = "black", fatten = 0.9, position = position_dodge(width = 0.75)) +
  facet_grid(corr + N ~ effect_size) +
  theme_minimal() +
  labs(
    title = "Inclusion Rate for effect size small across conditions",
    x = "Pattern of true effects",
    y = "Inclusion Rate",
    fill = "Method",
    color = "Method"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    strip.background = element_rect(colour = "black", fill = "white")
  )

p_rates_study2 <- combined_rates_by_effect_size_study2 %>%
  filter(!(heuristic %in% c("lower", "thres_lower", "thres_upper"))) %>%
  mutate(
    effect_size = factor(effect_size, levels = c("large", "medium", "small", "null")),
    method = dplyr::recode(heuristic,
                    "default" = "PPVS - SE heuristic upper bound",
                    "SSVS" = "SSVS"),
    N = factor(N, levels = c(50, 75), labels = c("N = 50", "N = 75")),
    effect_size = dplyr::recode(effect_size,
                         "large" = "Large effect size",
                         "medium" = "Medium effect size")
  ) %>%
  filter(!(effect_size %in% c("small", "null")), corr == 0.2) %>%
  ggplot(aes(x = factor(TE), y = inclusion_rate, fill = method, col = method)) + 
  geom_boxplot(alpha = 0.7) +
  stat_summary(fun = mean, geom = "crossbar", width = 0.8, color = "black", fatten = 0.9, position = position_dodge(width = 0.75)) +
  facet_grid(N ~ effect_size) +
  theme_minimal() +
  labs(
    title = "True inclusion Rate across conditions (ρ = 0.2)",
    x = "Pattern of true effects",
    y = "True inclusion Rate",
    fill = "Method",
    color = "Method"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    strip.background = element_rect(colour = "black", fill = "white")
  )

# save plot
ggsave("rates_effectsize_study2.png", plot = p_rates_study2, width = 10, height = 6, dpi = 300)
```

```{r}
p_rates_full_study2 <- combined_rates_by_effect_size_study2 %>%
  filter(!(heuristic %in% c("lower", "thres_lower", "thres_upper"))) %>%
  mutate(
    effect_size = factor(effect_size, levels = c("large", "medium", "small", "null")),
    method = dplyr::recode(heuristic,
                    "default" = "PPVS - SE heuristic upper bound",
                    "SSVS" = "SSVS"),
    N = factor(N, levels = c(50, 75), labels = c("N = 50", "N = 75")),
    corr = factor(corr, levels = c(0.2, 0.8), labels = c("ρ = 0.2", "ρ = 0.8")),
    effect_size = dplyr::recode(effect_size,
                         "small" = "Small effect size",
                         "null" = "Null effect size",
                         "medium" = "Medium effect size", 
                         "large" = "Large effect size")
  ) %>%
  ggplot(aes(x = factor(TE), y = inclusion_rate, fill = method, col = method)) + 
  geom_boxplot(alpha = 0.7) +
  stat_summary(fun = mean, geom = "crossbar", width = 0.8, color = "black", fatten = 0.9, position = position_dodge(width = 0.75)) +
  facet_grid(N  + corr~ effect_size) +
  theme_minimal() +
  labs(
    x = "Pattern of true effects",
    y = "Inclusion Rate",
    fill = "Method",
    color = "Method"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    strip.background = element_rect(colour = "black", fill = "white")
  )

# save plot
ggsave("rates_study2_full.png", plot = p_rates_full_study2, width = 10, height = 12, dpi = 300)
```

## where lower worked
```{r}
p_rates_subset_study2 <- combined_rates_by_effect_size_study2 %>%
  semi_join(combined_rates_by_effect_size_study2 %>%
              filter(heuristic == "lower", !is.na(inclusion_rate)) %>%
              select(N, corr, TE, r),
            by = c("N", "corr", "TE", "r")
  ) %>%
  mutate(
    effect_size = factor(effect_size, levels = c("large", "medium", "small", "null")),
    method = dplyr::recode(heuristic,
                    "default" = "PPVS - SE heuristic upper bound",
                    "lower" = "PPVS - SE heuristic lower bound",
                    "SSVS" = "SSVS"),
    N = factor(N, levels = c(50, 75), labels = c("N = 50", "N = 75")),
    corr = factor(corr, levels = c(0.2, 0.8), labels = c("ρ = 0.2", "ρ = 0.8")),
    effect_size = dplyr::recode(effect_size,
                         "small" = "Small effect size",
                         "null" = "Null effect size",
                         "medium" = "Medium effect size", 
                         "large" = "Large effect size")
  ) %>%
  filter(!(heuristic %in% c("thres_upper", "thres_lower"))) %>%
  ggplot(aes(x = factor(TE), y = inclusion_rate, fill = method, col = method)) + 
  geom_boxplot(alpha = 0.7) +
  stat_summary(fun = mean, geom = "crossbar", width = 0.8, color = "black", fatten = 0.9, position = position_dodge(width = 0.75)) +
  facet_grid(corr + N ~ effect_size) +
  theme_minimal() +
  labs(title = "Inclusion Rate cross conditions (subset)",
       x = "Pattern of true effects",
       y = "Inclusion Rate") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    strip.background = element_rect(colour = "black", fill = "white")
  ) + 
  scale_fill_manual(values = c(
    "PPVS - SE heuristic lower bound" = "#7CAE00",  # red
    "PPVS - SE heuristic upper bound" = "#F8766D",  # green
    "SSVS" = "#00BFC4"                              # blue
  )) +
  scale_color_manual(values = c(
    "PPVS - SE heuristic lower bound" = "#7CAE00",  # red
    "PPVS - SE heuristic upper bound" = "#F8766D",  # green
    "SSVS" = "#00BFC4"                              # blue
  ))


# save plot
ggsave("rates_study2_subset.png", plot = p_rates_subset_study2, width = 10, height = 12, dpi = 300)
```

# ppvs pmse
```{r cache=TRUE}
# Function to extract metadata from filenames
extract_metadata <- function(file_path) {
  data.frame(
    file_path = file_path,
    N = as.numeric(str_extract(file_path, "(?<=_N)\\d+")),
    corr = as.numeric(str_extract(file_path, "(?<=_corr)0\\.\\d+")),
    TE = str_extract(file_path, "(?<=_TE)[a-z]+"),
    r = as.numeric(str_extract(file_path, "(?<=_r)\\d+"))
  )
}

# Function to compute PMSE for each heuristic and prediction type
compute_pmse <- function(file_path) {
  load(file_path)  # Loads prj_pred_results_list

  metadata <- extract_metadata(file_path)
  y <- prj_pred_results_list$y
  heuristics <- names(prj_pred_results_list)[names(prj_pred_results_list) != "y"]
  expected_heuristics <- c("default", "lower", "thres_upper", "thres_lower")
  missing_heuristics <- setdiff(expected_heuristics, heuristics)

  pmse_results <- list()

  for (heuristic in expected_heuristics) {
    if (heuristic %in% heuristics) {
      preds <- prj_pred_results_list[[heuristic]]$prj_pred_testScaledOnTrain
      pmse <- mean((y - colMeans(preds))^2, na.rm = TRUE)
    } else {
      pmse <- NA_real_
    }

    pmse_results[[heuristic]] <- tibble(
      heuristic = heuristic,
      pmse = pmse
    )
  }

  return(bind_cols(metadata, bind_rows(pmse_results)))
}


# Get all prj_pred files
# setwd("D:/projpred_vs_spikeslab")
prj_pred_files_study2 <- list.files("output/output_study2/ppvs/prj_pred/", pattern = "^prj_pred_", full.names = TRUE)

# Apply function to all files
pmse_results_study2 <- bind_rows(lapply(prj_pred_files_study2, compute_pmse))
pmse_results_study2 <- pmse_results_study2[, -1]

# exclude reps with nonconver
pmse_results_study2 <- pmse_results_study2 %>%
  anti_join(nonconver_reps_study2, by = c("N", "corr", "TE", "r"))

# View results
print(pmse_results_study2)

```


# ssvs pmse
```{r cache=TRUE}
# List all SSVS prediction files
# setwd("D:/projpred_vs_spikeslab")
ssvs_pred_files_study2 <- list.files("output/output_study2/ssvs/ssvs_pred_results/", pattern = "^ssvs_pred_results_N", full.names = TRUE)

# Extract metadata from filenames
ssvs_metadata_study2 <- tibble(
  file = ssvs_pred_files_study2,
  N = as.numeric(str_extract(ssvs_pred_files_study2, "(?<=ssvs_pred_results_N)\\d+")),
  corr = as.numeric(str_extract(ssvs_pred_files_study2, "(?<=_corr)0\\.\\d+")),
  TE = str_extract(ssvs_pred_files_study2, "(?<=_TE)[a-z]+"),
  r = as.numeric(str_extract(ssvs_pred_files_study2, "(?<=_r)\\d+"))
)

# Function to compute PMSE
compute_pmse_ssvs_study2 <- function(file_path) {
  load(file_path)  # Loads ssvs_pred_results

  file_info <- ssvs_metadata_study2 %>% filter(file == file_path)
  y <- ssvs_pred_results$y
  preds <- ssvs_pred_results$pred_testScaledOnTrain

  pmse <- mean((y - colMeans(preds))^2)

  return(tibble(
    N = file_info$N,
    corr = file_info$corr,
    TE = file_info$TE,
    r = file_info$r,
    heuristic = "SSVS",
    pmse = pmse
  ))
}


# Compute PMSE for all files
ssvs_pmse_results_study2 <- bind_rows(lapply(ssvs_pred_files_study2, compute_pmse_ssvs_study2))

# View results
print(ssvs_pmse_results_study2)
```

# combine pmse
```{r}
# Combine both datasets
combined_pmse_study2 <- bind_rows(pmse_results_study2, ssvs_pmse_results_study2)
combined_pmse_study2$heuristic <- as.factor(combined_pmse_study2$heuristic)

# View structure to check
glimpse(combined_pmse_study2)
print(combined_pmse_study2)
```

# pmse visualization

```{r}
# Compute means
mean_pmse_study2 <- combined_pmse_study2 %>%
  filter(!(heuristic %in% c("lower", "thres_upper", "thres_lower"))) %>%
  mutate(method = dplyr::recode(heuristic,
                    "default" = "PPVS - SE heuristic upper bound",
                    "SSVS" = "SSVS")) %>%
  group_by(N, corr, TE, method) %>%
  summarise(mean_pmse = mean(pmse, na.rm = TRUE), .groups = "drop")

mean_pmse_study2

p_pmse_study2 <- combined_pmse_study2 %>%
  filter(!(heuristic %in% c("lower", "thres_lower", "thres_upper"))) %>%
  mutate(method = dplyr::recode(heuristic,
                    "default" = "PPVS - SE heuristic upper bound",
                    "SSVS" = "SSVS")) %>%
  ggplot(aes(x = pmse, fill = method, color = method)) +
  geom_density(alpha = 0.3) +  
  geom_vline(data = mean_pmse_study2,
             aes(xintercept = mean_pmse, color = method),
             linetype = "dashed", size = 0.8) +
  facet_grid(corr ~ TE + N, labeller = labeller(
      corr = c("0.2" = "ρ = 0.2", "0.8" = "ρ = 0.8"),
      TE = c("clustered" = "Clustered true effects", "mixed" = "Mixed true effects"),
      N = c("50" = "N = 50", "75" = "N = 75")
    )
  ) +  
  labs(x = "PMSE", 
       y = "Density") +
  theme_minimal() +
  theme(legend.position = "bottom",
    strip.background = element_rect(colour = "black", fill = "white"))
# save plot
ggsave("pmse_study2.png", plot = p_pmse_study2, width = 10, height = 6, dpi = 300)
```
## pmse where lower worked
```{r}
# filtering the replications where the lower threshold worked
p_pmse_subset_study2 <- combined_pmse_study2 %>%
  # filter for reps where 'lower' heuristic worked
  semi_join(
    combined_pmse_study2 %>%
      filter(heuristic == "lower", !is.na(pmse)) %>%
      select(N, corr, TE, r),
    by = c("N", "corr", "TE", "r")
  ) %>%
  filter(!(heuristic %in% c("thres_lower", "thres_upper"))) %>%
  mutate(
    method = dplyr::recode(heuristic,
                    "default" = "PPVS - SE heuristic upper bound",
                    "lower" = "PPVS - SE heuristic lower bound",
                    "SSVS" = "SSVS"),
    N = factor(N, levels = c(50, 75), labels = c("N = 50", "N = 75")),
    corr = factor(corr, levels = c(0.2, 0.8), labels = c("ρ = 0.2", "ρ = 0.8")),
    TE = dplyr::recode(TE,
                         "mixed" = "mixed true effects",
                         "clustered" = "clustered true effects")
  ) %>%
  ggplot(aes(x = factor(N), y = pmse, fill = method, col = method)) +
  geom_boxplot(alpha = 0.7) +  
  stat_summary(fun = mean, geom = "crossbar", width = 1, color = "black", fatten = 0.9, position = position_dodge(width = 0.75)) +
  facet_grid(TE ~ corr) +  
  labs(title = "PMSE Across Conditions (subset)", 
       x = "N", 
       y = "PMSE") +
  theme_minimal() + 
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    strip.background = element_rect(colour = "black", fill = "white")
  ) + 
  scale_fill_manual(values = c(
    "PPVS - SE heuristic lower bound" = "#7CAE00",  # red
    "PPVS - SE heuristic upper bound" = "#F8766D",  # green
    "SSVS" = "#00BFC4"                              # blue
  )) +
  scale_color_manual(values = c(
    "PPVS - SE heuristic lower bound" = "#7CAE00",  # red
    "PPVS - SE heuristic upper bound" = "#F8766D",  # green
    "SSVS" = "#00BFC4"                              # blue
  ))

# save plot
ggsave("pmse_subset_study2.png", plot = p_pmse_subset_study2, width = 10, height = 6, dpi = 300)
```


# CI coverage

## ppvs
```{r cache=TRUE}
# Define the function to compute mean and 95% HDI credible interval
summary_stats_mean_CI <- function(x) {
  mean_x <- mean(x)
  ci_x <- bayestestR::ci(x, method = "HDI", ci = 0.95)
  return(c(mean = mean_x, lower = ci_x$CI_low, upper = ci_x$CI_high))
}

# Define a function to process a single file (heuristic in one replication and condition)
compute_coverage <- function(file_path) {
  load(file_path)  # Loads prj_pred_results_list
  
  heuristics_found <- names(prj_pred_results_list)[names(prj_pred_results_list) != "y"]
  expected_heuristics <- c("default", "lower", "thres_upper", "thres_lower")
  missing_heuristics <- setdiff(expected_heuristics, heuristics_found)
  
  y_true <- prj_pred_results_list$y
  
  condition_info <- tibble(
    N = as.numeric(str_extract(file_path, "(?<=N)\\d+")),
    corr = as.numeric(str_extract(file_path, "(?<=_corr)0(?:\\.\\d+)?")),
    TE = str_extract(file_path, "(?<=_TE)[a-z]+"),
    r = as.numeric(str_extract(file_path, "(?<=_r)\\d+"))
  )
  
  results <- list()
  
  for (heuristic in heuristics_found) {
    pred_matrix <- prj_pred_results_list[[heuristic]]$prj_pred_testScaledOnTrain
    
    if (is.null(pred_matrix)) {
      results[[heuristic]] <- tibble(
        N = condition_info$N,
        corr = condition_info$corr,
        TE = condition_info$TE,
        r = condition_info$r,
        heuristic = heuristic,
        replication_coverage = NA_real_
      )
      next
    }
    
    # Compute lower and upper bounds for each test observation
    ci_bounds <- apply(pred_matrix, 2, summary_stats_mean_CI)
    result_df <- as.data.frame(t(ci_bounds))
    
    # Calculate proportion of observations covered
    replication_coverage <- mean((y_true > result_df$lower) & (y_true < result_df$upper))
    
    results[[heuristic]] <- tibble(
      N = condition_info$N,
      corr = condition_info$corr,
      TE = condition_info$TE,
      r = condition_info$r,
      heuristic = heuristic,
      replication_coverage = replication_coverage
    )
  }
  
  # Add NA rows for missing heuristics
  for (heuristic in missing_heuristics) {
    results[[heuristic]] <- tibble(
      N = condition_info$N,
      corr = condition_info$corr,
      TE = condition_info$TE,
      r = condition_info$r,
      heuristic = heuristic,
      replication_coverage = NA_real_
    )
  }
  
  return(bind_rows(results))
}

# Apply function to all files and combine results
ppvs_summary_coverage_study2 <- bind_rows(lapply(prj_pred_files_study2, compute_coverage))

# remove non-convergence replications
ppvs_summary_coverage_study2 <- ppvs_summary_coverage_study2 %>%
  anti_join(nonconver_reps_study2, by = c("N", "corr", "TE", "r"))

# View results
print(ppvs_summary_coverage_study2)
```



## ssvs
```{r cache=TRUE}
# Function to compute coverage for SSVS
compute_ssvs_coverage <- function(file) {
  load(file)  # Loads ssvs_pred_results
  
  # Extract metadata
  condition_info <- tibble(
    N = as.numeric(str_extract(file, "(?<=N)\\d+")),
    corr = as.numeric(str_extract(file, "(?<=_corr)0(?:\\.\\d+)?")),
    TE = str_extract(file, "(?<=_TE)[a-z]+"),
    r = as.numeric(str_extract(file, "(?<=_r)\\d+"))
  )
  
  y_true <- ssvs_pred_results$y
  pred_matrix <- ssvs_pred_results$pred_testScaledOnTrain

  # Compute coverage for each observation
  result <- apply(pred_matrix, 2, summary_stats_mean_CI)
  result_df <- as.data.frame(t(result))
  
   # Calculate proportion of observations covered
  replication_coverage <- mean((y_true > result_df$lower) & (y_true < result_df$upper))
  
  return(tibble(
    N = condition_info$N,
    corr = condition_info$corr,
    TE = condition_info$TE,
    r = condition_info$r,
    heuristic = "SSVS",
    replication_coverage = replication_coverage
  ))
}

# Apply function to all SSVS prediction files and combine results
system.time(
  ssvs_coverage_summary_study2 <- bind_rows(lapply(ssvs_pred_files_study2, compute_ssvs_coverage))
)

# View results
print(ssvs_coverage_summary_study2)
```

## combine coverage
```{r}
# Combine both datasets
combined_coverage_summary_study2 <- bind_rows(ppvs_summary_coverage_study2, ssvs_coverage_summary_study2)
combined_coverage_summary_study2$heuristic <- as.factor(combined_coverage_summary_study2$heuristic)

# View structure to check
glimpse(combined_coverage_summary_study2)
print(combined_coverage_summary_study2)
```


## coverage visualization
```{r}
combined_coverage_summary_study2 %>%
  filter(!(heuristic %in% c("lower", "SSVS", "thres_lower"))) %>%
  ggplot(aes(x = replication_coverage, fill = heuristic, col = heuristic)) + 
  geom_density(alpha = 0.3) +
  facet_grid(TE ~  corr + N) +  
  labs(title = "Density of coverage percentage for 100 replications Across Conditions", 
       x = "Replication coverage percentage", 
       y = "Density") +
  theme_minimal()

# Compute means
mean_percen_coverage_study2 <- combined_coverage_summary_study2 %>%
  filter(!(heuristic %in% c("lower", "thres_upper", "thres_lower"))) %>%
  mutate(method = dplyr::recode(heuristic,
                    "default" = "PPVS - SE heuristic upper bound",
                    "SSVS" = "SSVS")) %>%
  group_by(N, corr, TE, method) %>%
  summarise(mean_percen_coverage = mean(replication_coverage, na.rm = TRUE), .groups = "drop")

mean_percen_coverage_study2

p_coverage_percentage_study2 <- combined_coverage_summary_study2 %>%
  filter(!(heuristic %in% c("lower", "thres_upper", "thres_lower"))) %>%
    mutate(method = dplyr::recode(heuristic,
                    "default" = "PPVS - SE heuristic upper bound",
                    "SSVS" = "SSVS")) %>%
  ggplot(aes(x = replication_coverage, fill = method, col = method)) + 
  geom_density(alpha = 0.3) +
  geom_vline(data = mean_percen_coverage_study2,
             aes(xintercept = mean_percen_coverage, color = method),
             linetype = "dashed", size = 0.8) +
  geom_vline(xintercept = 0.95, linetype = "solid", color = "black", size = 0.7) + 
  facet_grid(corr ~ TE + N,
              labeller = labeller(
      corr = c("0.2" = "ρ = 0.2", "0.8" = "ρ = 0.8"),
      TE = c("clustered" = "Clustered true effects", "mixed" = "Mixed true effects"),
      N = c("50" = "N = 50", "75" = "N = 75")
    )
             ) +  
  labs(x = "Replication coverage percentage", 
       y = "Density") +
  theme_minimal() +
  theme(legend.position = "bottom",
    strip.background = element_rect(colour = "black", fill = "white"))
# save plot
ggsave("coverage_study2.png", plot = p_coverage_percentage_study2, width = 10, height = 6, dpi = 300)
```


### subset analysis
```{r}
# filtering the replications where the lower threshold worked
p_coverage_subset_study2 <- combined_coverage_summary_study2 %>%
  # filter for reps where 'lower' heuristic worked
  semi_join(
    combined_coverage_summary_study2 %>%
      filter(heuristic == "lower", !is.na(replication_coverage)) %>%
      select(N, corr, TE, r),
    by = c("N", "corr", "TE", "r")
  ) %>%

  filter(!(heuristic %in% c("thres_lower", "thres_upper"))) %>%
  mutate(
    method = dplyr::recode(heuristic,
                    "default" = "PPVS - SE heuristic upper bound",
                    "lower" = "PPVS - SE heuristic lower bound",
                    "SSVS" = "SSVS"),
    N = factor(N, levels = c(50, 75), labels = c("N = 50", "N = 75")),
    corr = factor(corr, levels = c(0.2, 0.8), labels = c("ρ = 0.2", "ρ = 0.8")),
    TE = dplyr::recode(TE,
                         "mixed" = "mixed true effects",
                         "clustered" = "clustered true effects")
  ) %>%
  ggplot(aes(x = factor(N), y = replication_coverage, fill = method, col = method)) +
  geom_boxplot(alpha = 0.7) +  
  stat_summary(fun = mean, geom = "crossbar", width = 1, color = "black", fatten = 0.9, position = position_dodge(width = 0.75)) +
  facet_grid(TE ~ corr) +  
  labs(title = "Replication coverage percentage across Conditions (subset)", 
       x = "N", 
       y = "Replication coverage percentage") +
  theme_minimal() + 
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    strip.background = element_rect(colour = "black", fill = "white")
  ) + 
  scale_fill_manual(values = c(
    "PPVS - SE heuristic lower bound" = "#7CAE00",  # red
    "PPVS - SE heuristic upper bound" = "#F8766D",  # green
    "SSVS" = "#00BFC4"                              # blue
  )) +
  scale_color_manual(values = c(
    "PPVS - SE heuristic lower bound" = "#7CAE00",  # red
    "PPVS - SE heuristic upper bound" = "#F8766D",  # green
    "SSVS" = "#00BFC4"                              # blue
  ))
# save plot
ggsave("coverage_subset_study2.png", plot = p_coverage_subset_study2, width = 10, height = 6, dpi = 300)
```

# computational time
## ppvs
```{r}
# Get all time files
time_files_study2 <- list.files("D:/projpred_vs_spikeslab/output/output_study2/ppvs/time_taken/", pattern = "^time_.*\\.RData$", full.names = TRUE)

# Initialize empty list to collect rows
time_data_study2 <- list()

# Loop through and process each file
for (file in time_files_study2) {
  load(file)  # loads time.taken

  # Extract filename components
  filename <- basename(file)
  N <- as.numeric(str_extract(filename, "(?<=time_N)\\d+"))
  corr <- as.numeric(str_extract(filename, "(?<=_corr)\\d+(\\.\\d+)?"))
  TE <- str_extract(filename, "(?<=_TE)[a-zA-Z]+")
  r <- as.numeric(str_extract(filename, "(?<=_r)\\d+"))

  # Append row
  time_data_study2[[length(time_data_study2) + 1]] <- data.frame(
    N = N,
    corr = corr,
    TE = TE,
    r = r,
    time_taken = as.numeric(time.taken, units = "secs"),
    method = "PPVS"
  )
}

# Combine all rows into a data frame
df_time_study2 <- bind_rows(time_data_study2)
df_time_study2
```

## ssvs
```{r}
# Get all time files
time_files_ssvs_study2 <- list.files("D:/projpred_vs_spikeslab/output/output_study2/ssvs/time_taken/", pattern = "^time_.*\\.RData$", full.names = TRUE)

# Initialize empty list to collect rows
time_data_ssvs_study2 <- list()

# Loop through and process each file
for (file in time_files_ssvs_study2) {
  load(file)  # loads time.taken

  # Extract filename components
  filename <- basename(file)
  N <- as.numeric(str_extract(filename, "(?<=time_N)\\d+"))
  corr <- as.numeric(str_extract(filename, "(?<=_corr)\\d+(\\.\\d+)?"))
  TE <- str_extract(filename, "(?<=_TE)[a-zA-Z]+")
  r <- as.numeric(str_extract(filename, "(?<=_r)\\d+"))

  # Append row
  time_data_ssvs_study2[[length(time_data_ssvs_study2) + 1]] <- data.frame(
    N = N,
    corr = corr,
    TE = TE,
    r = r,
    time_taken = as.numeric(time.taken, units = "secs"),
    method = "SSVS"
  )
}

# Combine all rows into a data frame
df_time_ssvs_study2 <- bind_rows(time_data_ssvs_study2)
df_time_ssvs_study2
```

## combined
```{r}
combined_time_study2 <- bind_rows(df_time_study2, df_time_ssvs_study2)
combined_time_study2 %>%
  group_by(N, corr, TE, method) %>%
  summarise(
    mean_time = round(mean(time_taken)/60, 2), 
    min_time = round(min(time_taken)/60, 2),
    sd_time = round(sd(time_taken)/60, 2),
    .groups = "drop"
  ) %>%
  arrange(method)
```

```{r}
# save(combined_rates_study2, combined_rates_by_effect_size_study2, combined_counts_study2, combined_pmse_study2, combined_coverage_summary_study2, combined_time_study2, nonconver_reps_study2, problematic_reps_study2, summary_na_study2, file = "result_objects_study2.RData")
```

